{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docker R Here we hope to learn about docker and build a container to run a big time R installation inside of ubuntu docker is made of 3 things: server, client, api server, dockerd * long running daemon **dockerd** * creates and manages docker objects, including: * containers * images * networks * volumes * swarms * listens to requests sent to the docker socket that can be mapped to another port or socket unix://var/run/docker.soc client, docker * command line tool, **docker**, sends commands to the server via the API * docker <--> API <--> dockerd * client commands are singular and precise: * run * create * restart * ..but the API is doing lots of work, for example, the API, when hearing **docker run**, will: * create a container, * ..and then create an image if it does not exist, * ..start the container once the image is built, * ..and then attach the container if its required, attach your terminals stdio to running container using containers ID or name api, glue between server and client, allows us to: * create, run and control containers * build, manage, commit, pull and push images * read and monitor logs from containers, * build and manage networks * create and control data volumes of persistent storage * provision docker swarms and scale services * the api connects the server, the docker engine **dockerd** to the client, **docker**, or, because its a RESTful API, to any client that implements parts of the API docker system is lots of things: container * houses all libraries and dependencies required to run in isolation * isolates is contents from its surroundings, allowing us to manage container on multiple platforms * shares host OS kernel, allowing it to start instantly and use less RAM hub * cloud based registry service * hosts code repos, workflows, pipelines, collaboration tools * search for and download docker images from ubuntu, apache, mysql, etc * can push and pull our own images * both **docker cloud** and **docker hub** require **docker id**, a docker hub account * collaboration means: * teams of users * automated builds with webhooks can be used within the build pipeline compose * defines and runs multi-container apps * configure multiple services which make up your application in one or more **docker compose files** * **docker compose files** are YAML * useful when deploying to different sets of environments, dev, prod, etc * we will use compose to manage and deploy projects to our local environment image * compiled **dockerfile** whicn can be built into many container instances * a set of layers that create instances of immutable and isolated containers log * retrieve a containers log file at time of execution * accessed via **compose** and through **docker** cli * standard unix STDOUT and STDERR can be accessed via **docker log** * more steps may be required to access the container logs: * a service may send log output to a file or database instead of a stream..these will be unavailable until logging output is configured * different logging drivers can be supplied to alter how logs are handled, thus allowing docker to access the logs network * by default, docker will create a network between running containers * network behaves as a bridge between host and containers * new containers are automatically connected to this default bridge when run * the default bridge supports port mapping to allow communication between containers * we can supply different options when creating containers that allow us to use user-defined networks instead * these user-defined networks can be configure to behave like bridge networks, overlay networks, or MAC VLANs volume * data volumes for persistent or shared data * means for sharing and storing persistent data * a **host directory** can be **bind mounted** or a **docker volume** can be used * volumes can be shard amongst containers * backup and copy features are available swarm * a cluster of docker engines configured on remote docker machines * allows to manage multiple docker engines and therefore control multiple services that have been deployed * replication, load balancing, rolling image and configuration updates, multi-host networking, cluster management are possible with swarm registry * app for storing and distributing images * search, commit, push pull machine * virtual host setup to run docker engine * like a VM with its own docker engine and therefore its own set of containers","title":"Home"},{"location":"#docker-r","text":"Here we hope to learn about docker and build a container to run a big time R installation inside of ubuntu","title":"Docker R"},{"location":"#docker-is-made-of-3-things-server-client-api","text":"","title":"docker is made of 3 things: server, client, api"},{"location":"#server-dockerd","text":"* long running daemon **dockerd** * creates and manages docker objects, including: * containers * images * networks * volumes * swarms * listens to requests sent to the docker socket that can be mapped to another port or socket unix://var/run/docker.soc","title":"server, dockerd"},{"location":"#client-docker","text":"* command line tool, **docker**, sends commands to the server via the API * docker <--> API <--> dockerd * client commands are singular and precise: * run * create * restart * ..but the API is doing lots of work, for example, the API, when hearing **docker run**, will: * create a container, * ..and then create an image if it does not exist, * ..start the container once the image is built, * ..and then attach the container if its required, attach your terminals stdio to running container using containers ID or name","title":"client, docker"},{"location":"#api-glue-between-server-and-client-allows-us-to","text":"* create, run and control containers * build, manage, commit, pull and push images * read and monitor logs from containers, * build and manage networks * create and control data volumes of persistent storage * provision docker swarms and scale services * the api connects the server, the docker engine **dockerd** to the client, **docker**, or, because its a RESTful API, to any client that implements parts of the API","title":"api, glue between server and client, allows us to:"},{"location":"#docker-system-is-lots-of-things","text":"","title":"docker system is lots of things:"},{"location":"#container","text":"* houses all libraries and dependencies required to run in isolation * isolates is contents from its surroundings, allowing us to manage container on multiple platforms * shares host OS kernel, allowing it to start instantly and use less RAM","title":"container"},{"location":"#hub","text":"* cloud based registry service * hosts code repos, workflows, pipelines, collaboration tools * search for and download docker images from ubuntu, apache, mysql, etc * can push and pull our own images * both **docker cloud** and **docker hub** require **docker id**, a docker hub account * collaboration means: * teams of users * automated builds with webhooks can be used within the build pipeline","title":"hub"},{"location":"#compose","text":"* defines and runs multi-container apps * configure multiple services which make up your application in one or more **docker compose files** * **docker compose files** are YAML * useful when deploying to different sets of environments, dev, prod, etc * we will use compose to manage and deploy projects to our local environment","title":"compose"},{"location":"#image","text":"* compiled **dockerfile** whicn can be built into many container instances * a set of layers that create instances of immutable and isolated containers","title":"image"},{"location":"#log","text":"* retrieve a containers log file at time of execution * accessed via **compose** and through **docker** cli * standard unix STDOUT and STDERR can be accessed via **docker log** * more steps may be required to access the container logs: * a service may send log output to a file or database instead of a stream..these will be unavailable until logging output is configured * different logging drivers can be supplied to alter how logs are handled, thus allowing docker to access the logs","title":"log"},{"location":"#network","text":"* by default, docker will create a network between running containers * network behaves as a bridge between host and containers * new containers are automatically connected to this default bridge when run * the default bridge supports port mapping to allow communication between containers * we can supply different options when creating containers that allow us to use user-defined networks instead * these user-defined networks can be configure to behave like bridge networks, overlay networks, or MAC VLANs","title":"network"},{"location":"#volume","text":"* data volumes for persistent or shared data * means for sharing and storing persistent data * a **host directory** can be **bind mounted** or a **docker volume** can be used * volumes can be shard amongst containers * backup and copy features are available","title":"volume"},{"location":"#swarm","text":"* a cluster of docker engines configured on remote docker machines * allows to manage multiple docker engines and therefore control multiple services that have been deployed * replication, load balancing, rolling image and configuration updates, multi-host networking, cluster management are possible with swarm","title":"swarm"},{"location":"#registry","text":"* app for storing and distributing images * search, commit, push pull","title":"registry"},{"location":"#machine","text":"* virtual host setup to run docker engine * like a VM with its own docker engine and therefore its own set of containers","title":"machine"},{"location":"dockerUbuntuR/dockerUbuntuR/","text":"Docker Ubuntu R:","title":"dockerUbuntuR"},{"location":"dockerUbuntuR/dockerUbuntuR/#docker-ubuntu-r","text":"","title":"Docker Ubuntu R:"},{"location":"dockerUbuntuR/installR/installR/","text":"ok","title":"installR"},{"location":"dockerUbuntuR/ubuntu2204base/dockerUbuntu/","text":"ok","title":"dockerUbuntu"},{"location":"learnDocker/learnDocker/","text":"Learning Docker:","title":"learnDocker"},{"location":"learnDocker/learnDocker/#learning-docker","text":"","title":"Learning Docker:"},{"location":"learnDocker/dockerConcepts/docker3things/","text":"docker is made of 3 things: server, client, api server, dockerd * long running daemon **dockerd** * creates and manages docker objects, including: * containers * images * networks * volumes * swarms * listens to requests sent to the docker socket that can be mapped to another port or socket unix://var/run/docker.soc client, docker * command line tool, **docker**, sends commands to the server via the API * docker <--> API <--> dockerd * client commands are singular and precise: * run * create * restart * ..but the API is doing lots of work, for example, the API, when hearing **docker run**, will: * create a container, * ..and then create an image if it does not exist, * ..start the container once the image is built, * ..and then attach the container if its required, attach your terminals stdio to running container using containers ID or name api, glue between server and client, allows us to: * create, run and control containers * build, manage, commit, pull and push images * read and monitor logs from containers, * build and manage networks * create and control data volumes of persistent storage * provision docker swarms and scale services * the api connects the server, the docker engine **dockerd** to the client, **docker**, or, because its a RESTful API, to any client that implements parts of the API","title":"Docker3things"},{"location":"learnDocker/dockerConcepts/docker3things/#docker-is-made-of-3-things-server-client-api","text":"","title":"docker is made of 3 things: server, client, api"},{"location":"learnDocker/dockerConcepts/docker3things/#server-dockerd","text":"* long running daemon **dockerd** * creates and manages docker objects, including: * containers * images * networks * volumes * swarms * listens to requests sent to the docker socket that can be mapped to another port or socket unix://var/run/docker.soc","title":"server, dockerd"},{"location":"learnDocker/dockerConcepts/docker3things/#client-docker","text":"* command line tool, **docker**, sends commands to the server via the API * docker <--> API <--> dockerd * client commands are singular and precise: * run * create * restart * ..but the API is doing lots of work, for example, the API, when hearing **docker run**, will: * create a container, * ..and then create an image if it does not exist, * ..start the container once the image is built, * ..and then attach the container if its required, attach your terminals stdio to running container using containers ID or name","title":"client, docker"},{"location":"learnDocker/dockerConcepts/docker3things/#api-glue-between-server-and-client-allows-us-to","text":"* create, run and control containers * build, manage, commit, pull and push images * read and monitor logs from containers, * build and manage networks * create and control data volumes of persistent storage * provision docker swarms and scale services * the api connects the server, the docker engine **dockerd** to the client, **docker**, or, because its a RESTful API, to any client that implements parts of the API","title":"api, glue between server and client, allows us to:"},{"location":"learnDocker/dockerConcepts/dockerSystem/","text":"docker system is lots of things: container * houses all libraries and dependencies required to run in isolation * isolates is contents from its surroundings, allowing us to manage container on multiple platforms * shares host OS kernel, allowing it to start instantly and use less RAM hub * cloud based registry service * hosts code repos, workflows, pipelines, collaboration tools * search for and download docker images from ubuntu, apache, mysql, etc * can push and pull our own images * both **docker cloud** and **docker hub** require **docker id**, a docker hub account * collaboration means: * teams of users * automated builds with webhooks can be used within the build pipeline compose * defines and runs multi-container apps * configure multiple services which make up your application in one or more **docker compose files** * **docker compose files** are YAML * useful when deploying to different sets of environments, dev, prod, etc * we will use compose to manage and deploy projects to our local environment image * compiled **dockerfile** whicn can be built into many container instances * a set of layers that create instances of immutable and isolated containers log * retrieve a containers log file at time of execution * accessed via **compose** and through **docker** cli * standard unix STDOUT and STDERR can be accessed via **docker log** * more steps may be required to access the container logs: * a service may send log output to a file or database instead of a stream..these will be unavailable until logging output is configured * different logging drivers can be supplied to alter how logs are handled, thus allowing docker to access the logs network * by default, docker will create a network between running containers * network behaves as a bridge between host and containers * new containers are automatically connected to this default bridge when run * the default bridge supports port mapping to allow communication between containers * we can supply different options when creating containers that allow us to use user-defined networks instead * these user-defined networks can be configure to behave like bridge networks, overlay networks, or MAC VLANs volume * data volumes for persistent or shared data * means for sharing and storing persistent data * a **host directory** can be **bind mounted** or a **docker volume** can be used * volumes can be shard amongst containers * backup and copy features are available swarm * a cluster of docker engines configured on remote docker machines * allows to manage multiple docker engines and therefore control multiple services that have been deployed * replication, load balancing, rolling image and configuration updates, multi-host networking, cluster management are possible with swarm registry * app for storing and distributing images * search, commit, push pull machine * virtual host setup to run docker engine * like a VM with its own docker engine and therefore its own set of containers","title":"dockerSystem"},{"location":"learnDocker/dockerConcepts/dockerSystem/#docker-system-is-lots-of-things","text":"","title":"docker system is lots of things:"},{"location":"learnDocker/dockerConcepts/dockerSystem/#container","text":"* houses all libraries and dependencies required to run in isolation * isolates is contents from its surroundings, allowing us to manage container on multiple platforms * shares host OS kernel, allowing it to start instantly and use less RAM","title":"container"},{"location":"learnDocker/dockerConcepts/dockerSystem/#hub","text":"* cloud based registry service * hosts code repos, workflows, pipelines, collaboration tools * search for and download docker images from ubuntu, apache, mysql, etc * can push and pull our own images * both **docker cloud** and **docker hub** require **docker id**, a docker hub account * collaboration means: * teams of users * automated builds with webhooks can be used within the build pipeline","title":"hub"},{"location":"learnDocker/dockerConcepts/dockerSystem/#compose","text":"* defines and runs multi-container apps * configure multiple services which make up your application in one or more **docker compose files** * **docker compose files** are YAML * useful when deploying to different sets of environments, dev, prod, etc * we will use compose to manage and deploy projects to our local environment","title":"compose"},{"location":"learnDocker/dockerConcepts/dockerSystem/#image","text":"* compiled **dockerfile** whicn can be built into many container instances * a set of layers that create instances of immutable and isolated containers","title":"image"},{"location":"learnDocker/dockerConcepts/dockerSystem/#log","text":"* retrieve a containers log file at time of execution * accessed via **compose** and through **docker** cli * standard unix STDOUT and STDERR can be accessed via **docker log** * more steps may be required to access the container logs: * a service may send log output to a file or database instead of a stream..these will be unavailable until logging output is configured * different logging drivers can be supplied to alter how logs are handled, thus allowing docker to access the logs","title":"log"},{"location":"learnDocker/dockerConcepts/dockerSystem/#network","text":"* by default, docker will create a network between running containers * network behaves as a bridge between host and containers * new containers are automatically connected to this default bridge when run * the default bridge supports port mapping to allow communication between containers * we can supply different options when creating containers that allow us to use user-defined networks instead * these user-defined networks can be configure to behave like bridge networks, overlay networks, or MAC VLANs","title":"network"},{"location":"learnDocker/dockerConcepts/dockerSystem/#volume","text":"* data volumes for persistent or shared data * means for sharing and storing persistent data * a **host directory** can be **bind mounted** or a **docker volume** can be used * volumes can be shard amongst containers * backup and copy features are available","title":"volume"},{"location":"learnDocker/dockerConcepts/dockerSystem/#swarm","text":"* a cluster of docker engines configured on remote docker machines * allows to manage multiple docker engines and therefore control multiple services that have been deployed * replication, load balancing, rolling image and configuration updates, multi-host networking, cluster management are possible with swarm","title":"swarm"},{"location":"learnDocker/dockerConcepts/dockerSystem/#registry","text":"* app for storing and distributing images * search, commit, push pull","title":"registry"},{"location":"learnDocker/dockerConcepts/dockerSystem/#machine","text":"* virtual host setup to run docker engine * like a VM with its own docker engine and therefore its own set of containers","title":"machine"}]}